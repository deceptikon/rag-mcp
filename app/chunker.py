import os
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter, Language


# === ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ ===

IGNORE_DIRS = {
    # Ð¡Ð±Ð¾Ñ€ÐºÐ° Ð¸ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸
    'node_modules', '.next', 'dist', 'build', '.vercel', 'node_modules',
    'venv', '.venv', '__pycache__', 'mediafiles', 'staticfiles', 'static',

    # ÐšÑÑˆÐ¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    '.ruff_cache', '.mypy_cache', '.pytest_cache', '.cursor', '.husky',
    '.git', '.github', '.vscode', '_TMP', '.brv', '.ci',

    # Ð¢ÐµÑÑ‚Ñ‹ (Ð½Ð° Ð¿ÐµÑ€Ð²Ð¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð»ÑƒÑ‡ÑˆÐµ ÑÐºÑ€Ñ‹Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿ÑƒÑ‚Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ)
    'cypress', 'tests'
}

IGNORE_FILES = {
    # Ð¡ÐµÐºÑ€ÐµÑ‚Ñ‹ Ð¸ ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð¸ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
    '.env', '.env.example', '.env.local', '.env.backup', '.env.production.local',
    'sample.env', 'db.sqlite3',

    # Ð›Ð¾Ðº-Ñ„Ð°Ð¹Ð»Ñ‹ (Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ðµ Ð¸ Ð±ÐµÑÐ¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð´Ð»Ñ RAG)
    'yarn.lock', 'package-lock.json', 'uv.lock', 'tsconfig.strict.tsbuildinfo',

    # ÐšÐ¾Ð½Ñ„Ð¸Ð³Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² (Ð»ÑƒÑ‡ÑˆÐµ ÑƒÐ±Ñ€Ð°Ñ‚ÑŒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð·Ð°Ð±Ð¸Ð²Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚)
    '.eslintrc.json', '.prettierrc.json', '.eslintignore', '.prettierignore',
    '.cursorignore', '.gitignore', '.dockerignore', '.vercelignore',
    'docker-compose.yml', 'Dockerfile', '.flake8', '.ruffignore'
}
# Ð£ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼, Ð¿Ð¾ ÐºÐ°ÐºÐ¸Ð¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼ Ð´ÐµÐ»Ð¸Ñ‚ÑŒ
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
# ÐšÐ°ÐºÐ¸Ðµ Ñ„Ð°Ð¹Ð»Ñ‹ Ð¼Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ð¼ "Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ"
ALLOWED_EXTENSIONS = {
    # Backend
    '.py': Language.PYTHON,
    # Frontend
    '.js': Language.JS,
    '.jsx': Language.JS,
    '.ts': Language.JS,
    '.tsx': Language.JS,
    # Docs & Configs
    '.md': Language.MARKDOWN,
    '.json': None, # Ð”Ð»Ñ ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð¾Ð², Ð½Ð¾ Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾
    '.sql': None,
    '': None # Ð”Ð»Ñ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð±ÐµÐ· Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ñ
}

def load_and_chunk_project(root_path):
    documents = []

    print(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐºÐ°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ: {os.path.abspath(root_path)}")

    for dirpath, dirnames, filenames in os.walk(root_path):
        # 1. Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ð¿Ð¾Ðº (ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð½ÐµÐ½ÑƒÐ¶Ð½Ñ‹Ðµ Ð¸Ð· Ð¾Ð±Ñ…Ð¾Ð´Ð°)
        dirnames[:] = [d for d in dirnames if d not in IGNORE_DIRS]

        for filename in filenames:
            if filename in IGNORE_FILES:
                continue

            file_ext = os.path.splitext(filename)[1]
            if file_ext not in ALLOWED_EXTENSIONS:
                continue

            full_path = os.path.join(dirpath, filename)
            relative_path = os.path.relpath(full_path, root_path)

            # 2. Ð§Ñ‚ÐµÐ½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð°
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except Exception as e:
                print(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ‡Ñ‚ÐµÐ½Ð¸Ñ {relative_path}: {e}")
                continue

            # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
            if not content.strip():
                continue

            # 3. Ð’Ñ‹Ð±Ð¾Ñ€ ÑÐ¿Ð»Ð¸Ñ‚Ñ‚ÐµÑ€Ð° Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ ÑÐ·Ñ‹ÐºÐ°
            language = ALLOWED_EXTENSIONS.get(file_ext)
            if file_ext == '.md':
                # Ð­Ð¢ÐÐŸ 1: Ð ÐµÐ¶ÐµÐ¼ Ð¿Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼
                md_header_chunks = md_header_splitter.split_text(content)

                # Ð­Ð¢ÐÐŸ 2: Ð”Ð¾Ñ€ÐµÐ·Ð°ÐµÐ¼ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ ÑÐµÐºÑ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»Ð° 5000 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)
                # Ð¸ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                final_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                chunks = final_splitter.split_documents(md_header_chunks)

                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ²Ð¾Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð¿ÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ) Ðº ÑƒÐ¶Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼
                for chunk in chunks:
                    chunk.metadata.update({
                        "source": relative_path,
                        "filename": filename,
                        "type": "documentation"
                    })

            if language:
                splitter = RecursiveCharacterTextSplitter.from_language(
                    language=language,
                    chunk_size=1000,
                    chunk_overlap=100
                )
            else:
                # Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¿Ð»Ð¸Ñ‚Ñ‚ÐµÑ€ Ð´Ð»Ñ SQL, TXT Ð¸ Ð¿Ñ€Ð¾Ñ‡ÐµÐ³Ð¾
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=100
                )

            # 4. ÐÐ°Ñ€ÐµÐ·ÐºÐ° Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸
            # ÐœÑ‹ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¡Ð ÐÐ—Ð£, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð½Ðµ Ð¿Ð¾Ñ‚ÐµÑ€ÑÑ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
            chunks = splitter.create_documents(
                [content],
                metadatas=[{
                    "source": relative_path,
                    "filename": filename,
                    "extension": file_ext
                }]
            )

            documents.extend(chunks)
            print(f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½: {relative_path} -> {len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð²")

    return documents
