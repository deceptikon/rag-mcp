import sys
import os
import logging
import json
import ollama
from typing import Optional
from mcp.server.fastmcp import FastMCP
from app.chunker import load_and_chunk_project
from app.vector_store import VectorStoreManager
from sentence_transformers import CrossEncoder
import numpy as np

# Setup Logging to be actually visible
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("vox-brain")

# Initialize FastMCP Server
mcp = FastMCP("VOX Brain (RAG)", dependencies=["chromadb", "langchain-text-splitters"])


def get_collection_name(project_path: str) -> str:
    """Gets the collection name from the project path."""
    project_name = os.path.basename(os.path.abspath(project_path))
    return f"rag-mcp-{project_name}"


@mcp.tool()
async def index_project(project_path: str, model: str = "nomic-embed-text") -> str:
    """
    Indexes a project directory into the vector store.
    Args:
        project_path: Absolute path to the project root.
        model: The embedding model to use.
    """
    logger.info(f"üöÄ Starting indexing for: {project_path}")

    if not os.path.isdir(project_path):
        logger.error(f"‚ùå Invalid path: {project_path}")
        return f"Error: {project_path} is not a valid directory."

    try:
        collection_name = get_collection_name(project_path)
        logger.info(f"üì¶ Loading and chunking files...")
        documents = load_and_chunk_project(project_path)

        logger.info(
            f"üíæ Upserting {len(documents)} chunks to collection: {collection_name}"
        )
        db_path="$project_path/.rag_db"
        vector_store = VectorStoreManager(
            db_path=db_path, collection_name=collection_name
        )
        vector_store.add_documents(documents, model=model)

        logger.info(f"‚úÖ Indexing complete for {project_path}")
        sys.stdout.flush()
        return f"Successfully indexed {len(documents)} chunks into {collection_name}."
    except Exception as e:
        logger.exception("üî• Indexing failed")
        return f"Indexing failed: {str(e)}"


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ—Ä–∞–Ω–∫–µ—Ä (–ª—É—á—à–µ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∏–ª–∏ —Å–∏–Ω–≥–ª—Ç–æ–Ω)
# –ú–æ–¥–µ–ª—å ms-marco-MiniLM-L-6-v2 ‚Äî –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç: –±—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è.
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

#async def rewrite_query_for_code(query: str) -> str:
#    prompt = f"Given the programming question: '{query}', list 3-5 technical keywords or function names that might appear in the source code. Output only keywords separated by commas."
#    # –í—ã–∑—ã–≤–∞–µ–º Ollama gemma3:4b-it-qat (–æ–Ω–∞ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è, —ç—Ç–æ –∑–∞–π–º–µ—Ç <1 —Å–µ–∫)
#
#    response = ollama.chat(
#        model=model,
#        messages=[
#            {"role": "system", "content": system_prompt},
#            {"role": "user", "content": user_prompt},
#        ],
#        options={
#            "temperature": 0.0,  # –î–µ–ª–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –∏ —Ç–æ—á–Ω—ã–º–∏
#            "num_ctx": 8192,     # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ, —á—Ç–æ–±—ã –≤–ª–µ–∑–ª–æ –±–æ–ª—å—à–µ —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048)
#            "num_predict": 1024  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞, —á—Ç–æ–±—ã —ç–∫–æ–Ω–æ–º–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
#        }
#    )
#    keywords = response["message"]["content"]    # –†–µ–∑—É–ª—å—Ç–∞—Ç: "AuthService, JWT, login, authenticate, token"
#    return keywords

async def rewrite_query_for_code(query: str) -> str:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∂–∞—Ç—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_msg = "You are a technical search assistant. Output ONLY a comma-separated list of technical terms. No prose."
    user_msg = f"Translate this intent into code keywords: '{query}'"

    try:
        response = ollama.chat(
            model="gemma3:4b-it-qat",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            options={
                "temperature": 0.1,    # –ú–∏–Ω–∏–º—É–º –∫—Ä–µ–∞—Ç–∏–≤–∞
                "num_predict": 30,     # –ù–∞–º –Ω—É–∂–Ω–æ –≤—Å–µ–≥–æ –ø–∞—Ä—É —Å–ª–æ–≤, –Ω–µ –¥–∞–µ–º –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å
                "stop": ["\n", "Sure", "Here"] # –û–±—Ä–µ–∑–∞–µ–º –ª–∏—à–Ω—é—é –≤–µ–∂–ª–∏–≤–æ—Å—Ç—å
            }
        )
        keywords = response["message"]["content"].strip()
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞ (–∫–∞–≤—ã—á–∫–∏, —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ)
        return keywords.replace('"', '').replace('.', '')
    except Exception as e:
        logger.warning(f"Query expansion failed: {e}")
        return "" # –ï—Å–ª–∏ —É–ø–∞–ª–æ, –ø–æ–∏—Å–∫ –ø–æ–π–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É

@mcp.tool()
async def search_project(project_path: str, query: str, top_k: int = 20) -> str:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è Gemma 3.
    """
    # 1. Query Expansion (–ù–ï –∑–∞–º–µ–Ω—è–µ–º, –∞ —Ä–∞—Å—à–∏—Ä—è–µ–º)
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –∫–æ–¥–µ
    search_terms = await rewrite_query_for_code(query)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º—ã –∏—â–µ–º –∏ –ø–æ —Å–º—ã—Å–ª—É, –∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    combined_query = f"{query} {search_terms}"

    logger.info(f"üîé Combined Query: {combined_query}")

    collection_name = get_collection_name(project_path)
    vector_store = VectorStoreManager(
        db_path=".rag_db", collection_name=collection_name
    )

    try:
        # 1. Retrieval: –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º (20 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤), —á—Ç–æ–±—ã –±—ã–ª–æ –∏–∑ —á–µ–≥–æ –≤—ã–±–∏—Ä–∞—Ç—å
        initial_results = vector_store.search(
            query_text=combined_query,  # –ò—â–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
            model="nomic-embed-text",
            n_results=20
        )

        if not initial_results:
            return "No relevant results found."

        # 2. Reranking: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –∫–∞–∂–¥—ã–º –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫—É—Å–∫–æ–º –∫–æ–¥–∞
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä—ã [–≤–æ–ø—Ä–æ—Å, –∫–æ–¥] –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        pairs = [[query, res.get('content', '')] for res in initial_results]
        scores = reranker.predict(pairs)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∫–æ—Ä—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        for i, res in enumerate(initial_results):
            res['rerank_score'] = scores[i]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º –Ω—É–∂–Ω—ã–µ top_k
        reranked_results = sorted(
            initial_results, key=lambda x: x['rerank_score'], reverse=True
        )[:top_k]

        # 3. Formatting: –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ XML-–ø–æ–¥–æ–±–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        formatted_output = ["<context>"]

        for i, res in enumerate(reranked_results):
            source = res.get('source', 'unknown')
            content = res.get('content', '')
            # –ß–µ—Ç–∫–æ –æ—Ç–¥–µ–ª—è–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            doc_block = (
                f"### DOCUMENT {i+1}\n"
                f"FILE_PATH: {source}\n"
                f"CODE_CONTENT:\n{content}\n"
                f"--- END OF DOCUMENT {i+1} ---"
            )
            formatted_output.append(doc_block)

        formatted_output.append("</context>")

        logger.info(f"‚ú® Reranking complete. Best score: {reranked_results[0].get('rerank_score'):.4f}")
        return "\n\n".join(formatted_output)

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Search/Rerank failed: {str(e)}")
        return f"Search error: {str(e)}"



@mcp.tool()
async def ask_project(
    project_path: str, question: str, model: str = "gemma3:4b-it-qat"
) -> str:
    """
    Asks a question about the project, using retrieved context to answer.
    Args:
        project_path: Absolute path to the project root.
        question: The user's question.
        model: The Ollama model to use for generation (default: gemma3:4b-it-qat).
    """
    logger.info(f"ü§î Asking '{model}' about {project_path}: '{question}'")

    # 1. Reuse search logic to get context
    context = await search_project(project_path, question)

    if context.startswith("Search error") or context == "No relevant results found.":
        return f"Could not retrieve context to answer the question. Reason: {context}"

    # 2. Construct Prompt
#    system_prompt = (
#        "You are an expert software engineer assistant. "
#        "Answer the user's question based strictly on the provided code context. "
#        "If the answer is not in the context, state that you don't know."
#    )
    system_prompt = (
        "You are an expert software engineer assistant specializing in RAG systems. "
        "Your task is to answer the user's question based ONLY on the provided code context. "
        "For every piece of information you provide, you MUST cite the source file using the format [SOURCE: filename]. "
    )

    #user_prompt = f"Context:\n{context}\n\nQuestion: {question}\n\nAnswer:"
    user_prompt = f"""
    ### SOURCE CODE CONTEXT
    --------------------------------------------------
    {context}
    --------------------------------------------------

    ### USER QUESTION
    {question}

    ### ANSWER
    """

    user_prompt += (
        "#### Response Instructions:\n"
        "1. Analyze the context provided above and identify the key snippets that directly relate to the question.\n"
        "2. Provide a detailed answer using ONLY information from these snippets.\n"
        "3. Cite the source file for each key assertion using the format [SOURCE: filename].\n"
        "4. If information is missing, EXPLICITLY reply that you refuse to answer the specific part that is missing context.\n\n"
        "#### Answer:"
    )

    try:
        # 3. Call Ollama
        response = ollama.chat(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            options={
                "temperature": 0.0,  # –î–µ–ª–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –∏ —Ç–æ—á–Ω—ã–º–∏
                "num_ctx": 8192,     # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ, —á—Ç–æ–±—ã –≤–ª–µ–∑–ª–æ –±–æ–ª—å—à–µ —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048)
                "num_predict": 1024  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞, —á—Ç–æ–±—ã —ç–∫–æ–Ω–æ–º–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã
            }
        )
        answer = response["message"]["content"]
        logger.info("üí° Answer generated successfully")
        return answer
    except Exception as e:
        logger.exception("üî• LLM generation failed")
        return f"Error generating answer: {str(e)}"


@mcp.resource("vox://{project_id}/rules")
def get_project_rules(project_id: str) -> str:
    """Reads the project rules from the context store."""
    docs_path = os.path.expanduser(f"$VOX_HOME/context/docs/{project_id}/docs.jsonl")
    logger.info(f"üìú Reading rules from: {docs_path}")

    if not os.path.exists(docs_path):
        return "No rules found for this project."

    rules = []
    try:
        with open(docs_path, "r") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if data.get("type") == "rule":
                        rules.append(f"### {data.get('title')}\n{data.get('content')}")
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        return f"Error reading rules: {str(e)}"

    return "\n\n".join(rules) if rules else "No rules found."


@mcp.resource("vox://{project_id}/docs")
def get_project_docs(project_id: str) -> str:
    """Reads the project documentation from the context store."""
    docs_path = os.path.expanduser(f"$VOX_HOME/context/docs/{project_id}/docs.jsonl")
    logger.info(f"üìú Reading docs from: {docs_path}")

    if not os.path.exists(docs_path):
        return "No documentation found for this project."

    docs = []
    try:
        with open(docs_path, "r") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if data.get("type") == "doc":
                        docs.append(f"### {data.get('title')}\n{data.get('content')}")
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        return f"Error reading docs: {str(e)}"

    return "\n\n".join(docs) if docs else "No documentation found."


@mcp.resource("vox://{project_id}/notes")
def get_project_notes(project_id: str) -> str:
    """Reads the project notes from the context store."""
    docs_path = os.path.expanduser(f"$VOX_HOME/context/docs/{project_id}/docs.jsonl")
    logger.info(f"üìú Reading notes from: {docs_path}")

    if not os.path.exists(docs_path):
        return "No notes found for this project."

    notes = []
    try:
        with open(docs_path, "r") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if data.get("type") == "note":
                        notes.append(f"### {data.get('title')}\n{data.get('content')}")
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        return f"Error reading notes: {str(e)}"

    return "\n\n".join(notes) if notes else "No notes found."


@mcp.resource("vox://{project_id}/tree")
def get_project_tree(project_id: str) -> str:
    """Gets the file tree structure of the project."""
    # We need to resolve the project_id to a path first.
    # We can use the $VOX_HOME/context/projects/{id}/config.json file
    config_path = os.path.expanduser(
        f"$VOX_HOME/context/projects/{project_id}/config.json"
    )

    if not os.path.exists(config_path):
        return f"Project {project_id} not found."

    try:
        with open(config_path, "r") as f:
            config = json.load(f)
            project_path = config.get("path")
    except Exception as e:
        return f"Error reading project config: {str(e)}"

    if not project_path or not os.path.exists(project_path):
        return f"Project path not found: {project_path}"

    logger.info(f"üå≥ Generating tree for: {project_path}")

    tree_lines = []
    try:
        for root, dirs, files in os.walk(project_path):
            # Filter excluded directories (basic)
            dirs[:] = [
                d
                for d in dirs
                if d
                not in {
                    ".git",
                    "node_modules",
                    "__pycache__",
                    ".venv",
                    "venv",
                    "dist",
                    "build",
                }
            ]

            level = root.replace(project_path, "").count(os.sep)
            indent = " " * 4 * (level)
            tree_lines.append(f"{indent}{os.path.basename(root)}/")
            subindent = " " * 4 * (level + 1)
            for f in files:
                if not f.startswith("."):  # Skip hidden files
                    tree_lines.append(f"{subindent}{f}")

        return "\n".join(tree_lines)
    except Exception as e:
        return f"Error generating tree: {str(e)}"


if __name__ == "__main__":
    # When run directly, use Stdio transport (default for FastMCP)
    mcp.run()
